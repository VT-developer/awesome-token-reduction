# Awesome Token Reduction Papers

This repository contains a list of recent papers on token reduction for ML/AI; we categorize them based on their year and application scenarios.

We will try to make this list updated. If you found any error or any missed paper, please don't hesitate to open an issue or pull request.


## üî• News
Last update on 2024/10/22

## üåÅ Vision 
#### 2024
* [arXiv 2024] Dynamic and Compressive Adaptation of Transformers From Images to Videos [[Paper](https://arxiv.org/pdf/2408.06840)]
* [ECCV 2024] Agglomerative Token Clustering [[Paper](https://arxiv.org/pdf/2409.11923)][[Code](https://github.com/JoakimHaurum/ATC)] 
* [ECCV 2024] Token Compensator: Altering Inference Cost of Vision Transformer without Re-Tuning [[Paper](https://arxiv.org/pdf/2408.06798)][[Code](https://github.com/JieShibo/ToCom)]
* [ECCV 2024] LookupViT: Compressing visual information to a limited number of tokens [[Paper](https://arxiv.org/pdf/2407.12753)]
* [ECCV 2024] PYRA: Parallel Yielding Re-Activation for Training-Inference Efficient Task Adaptation [[Paper](https://arxiv.org/abs/2403.09192)][[Code](https://github.com/THU-MIG/PYRA?tab=readme-ov-file)]

#### 2023
* [arXiv 2023] PPT: Token Pruning and Pooling for Efficient Vision Transformers [[Paper](https://arxiv.org/pdf/2310.01812)][[Code](https://github.com/xjwu1024/PPT)]
* [ICCV 2023] Dynamic Token Pruning in Plain Vision Transformers for Semantic Segmentation [[Paper](https://openaccess.thecvf.com/content/ICCV2023/papers/Tang_Dynamic_Token_Pruning_in_Plain_Vision_Transformers_for_Semantic_Segmentation_ICCV_2023_paper.pdf)][[Code](https://github.com/zbwxp/Dynamic-Token-Pruning)]
* [ICCV 2023] DiffRate: Differentiable Compression Rate for Efficient Vision Transformers [[Paper](https://arxiv.org/abs/2305.17997)][[Code](https://github.com/OpenGVLab/DiffRate)]
* [ICCV 2023] TORE: Token Reduction for Efficient Human Mesh Recovery with Transformer [[Paper](https://openaccess.thecvf.com/content/ICCV2023/papers/Dou_TORE_Token_Reduction_for_Efficient_Human_Mesh_Recovery_with_Transformer_ICCV_2023_paper.pdf)][[Code](https://github.com/Frank-ZY-Dou/TORE)] 
* [ICCV 2023 Workshop] Which Tokens to Use? Investigating Token Reduction in Vision Transformers [[Paper](https://arxiv.org/abs/2308.04657)][[Code](https://github.com/JoakimHaurum/TokenReduction)] 
* [CVPR 2023] Joint Token Pruning and Squeezing Towards More Aggressive Compression of Vision Transformers [[Paper](https://arxiv.org/pdf/2304.10716)][[Code](https://github.com/megvii-research/TPS-CVPR2023)]
* [ICLR 2023] Token Merging: Your ViT But Faster [[Paper](https://arxiv.org/pdf/2210.09461)][[Code](https://github.com/facebookresearch/ToMe)]
* [IJCAI 2023] Adaptive Sparse ViT: Towards Learnable Adaptive Token Pruning by Fully Exploiting Self-Attention [[Paper](https://arxiv.org/pdf/2209.13802)][[Code](https://github.com/Cydia2018/AS-ViT)]
* [TIP] https://github.com/zbwxp/Dynamic-Token-Pruning [[Paper](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10183862)] 

#### 2022
* [ECCV 2022] SPViT: Enabling Faster Vision Transformers via Latency-aware Soft Token Pruning [[Paper](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136710618.pdf)][[Code](https://github.com/PeiyanFlying/SPViT)] 
* [ECCV 2022] ATS: Adaptive Token Sampling For Efficient Vision Transformers [[Paper](https://arxiv.org/abs/2111.15667)][[Code](https://github.com/adaptivetokensampling/ATS)]
* [ECCV 2022] PPT: token-Pruned Pose Transformer for monocular and multi-view human pose estimation [[Paper](https://arxiv.org/pdf/2209.08194)][[Code](https://github.com/HowieMa/PPT)]
* [CVPR 2022] Vision Transformer Slimming: Multi-Dimension Searching in Continuous Optimization Space [[Paper](https://arxiv.org/pdf/2201.00814)][[Code](https://github.com/Arnav0400/ViT-Slim)]
* [CVPR 2022] Patch Slimming for Efficient Vision Transformers [[Paper](https://arxiv.org/abs/2106.02852)]
* [CVPR 2022] A-ViT: Adaptive Tokens for Efficient Vision Transformer [[Paper](https://arxiv.org/pdf/2112.07658)][[Code](https://github.com/NVlabs/A-ViT)]
* [ICLR 2022] EViT: Expediting Vision Transformers via Token Reorganizations [[Paper](https://arxiv.org/pdf/2202.07800)][[Code](https://github.com/youweiliang/evit?tab=readme-ov-file)]
* [AAAI 2022] Evo-ViT: Slow-Fast Token Evolution for Dynamic Vision Transformer [[Paper](https://arxiv.org/abs/2108.01390)][[Code](https://github.com/YifanXu74/Evo-ViT)]

#### 2021
* [NeurIPS 2021] DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification [[Paper](https://arxiv.org/abs/2106.02034)][[Code](https://github.com/raoyongming/DynamicViT)]


## üìù Language 
#### 2023
* [EMNLP 2023] Optimizing Retrieval-augmented Reader Models via Token Elimination [[Paper](https://arxiv.org/pdf/2310.13682)][[Code](https://github.com/IntelLabs/token_elimination)]
* [EMNLP 2023] Context Compression for Auto-regressive Transformers with Sentinel Tokens [[Paper](https://arxiv.org/pdf/2310.08152)][[Code](https://github.com/DRSY/KV_Compression)] 
* [EMNLP 2023] Leap-of-Thought: Accelerating Transformers via Dynamic Token Routing [[Paper](https://aclanthology.org/2023.emnlp-main.976.pdf)][[Code](https://github.com/yeachan-kr/lot)]  
* [EMNLP 2023] TLM: Token-Level Masking for Transformers [[Paper](https://arxiv.org/pdf/2310.18738)][[Code](https://github.com/Young1993/tlm)]  
* [EMNLP 2023] Understanding the Role of Input Token Characters in Language Models: How Does Information Loss Affect Performance? [[Paper](https://aclanthology.org/2023.emnlp-main.563.pdf)]  
* [ACL 2023] Going Beyond Sentence Embeddings: A Token-Level Matching Algorithm for Calculating Semantic Textual Similarity [[Paper](https://aclanthology.org/2023.acl-short.49.pdf)]
* [ACL 2023] Efficient Transformers with Dynamic Token Pooling [[Paper](https://aclanthology.org/2023.acl-long.353.pdf)]
* [ACL 2023] Token-wise Decomposition of Autoregressive Language Model Hidden States for Analyzing Model Predictions [[Paper](https://aclanthology.org/2023.acl-long.562.pdf)]
* [ACL 2023] LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models [[Paper](https://arxiv.org/pdf/2310.05736)][[Code](https://github.com/microsoft/LLMLingua)]  
* [ACL 2023] UTC-IE: A Unified Token-pair Classification Architecture for Information Extraction [[Paper](https://aclanthology.org/2023.acl-long.226.pdf)] 
* [ACL 2023] Revisiting Token Dropping Strategy in Efficient BERT Pretraining [[Paper](https://aclanthology.org/2023.acl-long.579.pdf)]
#### 2022
* [ACL 2022] Pyramid-BERT: Reducing Complexity via Successive Core-set based Token Selection [[Paper](https://aclanthology.org/2022.acl-long.602.pdf)]
* [ACL 2022] AdapLeR: Speeding up Inference by Adaptive Length Reduction [[Paper](https://aclanthology.org/2022.acl-long.1.pdf)][[Code](https://github.com/amodaresi/AdapLeR)]   
* [KDD 2022] Learned Token Pruning for Transformers [[Paper](https://arxiv.org/pdf/2107.00910)][[Code](https://github.com/kssteven418/LTP)]    


## üé¨ Vision-Language Model 
#### 2024
* [arXiv 2024] TokenPacker: Efficient Visual Projector for Multimodal LLM [[Paper](https://arxiv.org/pdf/2407.02392)][[Code](https://github.com/CircleRadon/TokenPacker)]
* [arXiv 2024] Less is More: A Simple yet Effective Token Reduction Method for Efficient Multi-modal LLMs [[Paper](https://arxiv.org/pdf/2409.10994)]
* [arXiv 2024] TG-LLaVA: Text Guided LLaVA via Learnable Latent Embeddings [[Paper](https://arxiv.org/pdf/2409.09564)]
* [arXiv 2024] mPLUG-DocOwl2: High-resolution Compressing for OCR-free Multi-page Document Understanding [[Paper](https://arxiv.org/abs/2409.03420)][[Code](https://github.com/X-PLUG/mPLUG-DocOwl)]
* [arXiv 2024] TempMe: Video Temporal Token Merging for Efficient Text-Video Retrieval [[Paper](https://arxiv.org/pdf/2409.01156)][[Code](https://github.com/X-PLUG/mPLUG-DocOwl)]
* [arXiv 2024] Recoverable Compression: A Multimodal Vision Token Recovery Mechanism Guided by Text Information [[Paper](https://arxiv.org/pdf/2409.01179)]
* [arXiv 2024] HiRED: Attention-Guided Token Dropping for Efficient Inference of High-Resolution Vision-Language Models in Resource-Constrained Environments [[Paper](https://arxiv.org/pdf/2408.10945)][[Code](https://github.com/hasanar1f/HiRED)]
* [arXiv 2024] Token-level Correlation-guided Compression for Efficient Multimodal Document Understanding [[Paper](https://arxiv.org/pdf/2407.14439)][[Code](https://github.com/JiuTian-VL/TokenCorrCompressor)]
* [arXiv 2024] HiRes-LLaVA: Restoring Fragmentation Input in High-Resolution Large Vision-Language Models [[Paper](https://arxiv.org/pdf/2407.08706)]
* [arXiv 2024] TokenPacker: Efficient Visual Projector for Multimodal LLM [[Paper](https://arxiv.org/abs/2407.02392.pdf)][[Code](https://github.com/CircleRadon/TokenPacker)]
* [arXiv 2024] VoCo-LLaMA: Towards Vision Compression with Large Language Models [[Paper](https://arxiv.org/pdf/2406.12275)][[Code](https://github.com/Yxxxb/VoCo-LLaMA)]
* [arXiv 2024] DeCo: Decoupling Token Compression from Semantic Abstraction in Multimodal Large Language Models [[Paper](https://arxiv.org/pdf/2405.20985)][[Code](https://github.com/yaolinli/DeCo)]
* [arXiv 2024] CATP: Cross-Attention Token Pruning for Accuracy Preserved Multimodal Model Inference [[Paper](https://arxiv.org/pdf/2404.08567)]
* [arXiv 2024] MobileVLM V2: Faster and Stronger Baseline for Vision Language Model [[Paper](https://arxiv.org/abs/2402.03766.pdf)][[Code](https://github.com/Meituan-AutoML/MobileVLM)]
* [arXiv 2024] LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models [[Paper](https://arxiv.org/abs/2403.15388.pdf)][[Code](https://github.com/42Shawn/LLaVA-PruMerge)]
* [ECCV 2024] IVTP: Instruction-guided Visual Token Pruning for Large Vision-Language Models [[Paper](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02577.pdf)]
* [ECCV 2024] An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Acceleration for VLLM Inference [[Paper](https://arxiv.org/pdf/2403.06764)][[Code](https://github.com/pkunlp-icler/FastV)]
* [ICML 2024] CrossGET: Cross-Guided Ensemble of Tokens for Accelerating Vision-Language Transformers [[Paper](https://arxiv.org/pdf/2305.17455v4)][[Code](https://github.com/sdc17/CrossGET)]
* [ECCV 2024] LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models [[Paper](https://arxiv.org/abs/2311.17043)][[Code](https://github.com/dvlab-research/LLaMA-VID/tree/main)]
* [CVPR 2024] Honeybee: Locality-enhanced Projector for Multimodal LLM [[Paper](https://arxiv.org/abs/2312.06742)][[Code](https://github.com/khanrc/honeybee?tab=readme-ov-file)]
#### 2023
* [arXiv 2023] Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond [[Paper](https://arxiv.org/pdf/2308.12966v2)][[Code](https://github.com/QwenLM/Qwen-VL)]
* [arXiv 2023] BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models [[Paper](https://arxiv.org/abs/2301.12597)][[Code](https://github.com/salesforce/LAVIS/tree/main/projects/blip2)]
* [ACL 2023] PuMer: Pruning and Merging Tokens for Efficient Vision Language Models [[Paper](https://aclanthology.org/2023.acl-long.721.pdf)][[Code](https://github.com/csarron/PuMer)]  

## üêç State Space Models 
* [EMNLP 2024] Rethinking Token Reduction for State Space Models [[Paper](https://arxiv.org/pdf/2410.14725)][[Code](https://github.com/wuyushuwys/ToR_SSM)]
* [NeurIPS 2024] Exploring Token Pruning in Vision State Space Models [[Paper](https://arxiv.org/pdf/2409.18962)]


## üì± Hardware Co-design 
* [HPCA 2023] HeatViT: Hardware-Efficient Adaptive Token Pruning for Vision Transformers [[Paper](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10071047)]
* [HPCA 2021] SpAtten: Efficient Sparse Attention Architecture with Cascade Token and Head Pruning [[Paper](https://arxiv.org/pdf/2012.09852)][[Code](https://github.com/mit-han-lab/spatten)]
